\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage{format}
\usepackage{commands}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Extended abstract}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.}
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{ema\\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }

\author{Simon Théorêt}
\begin{document}

\maketitle

\begin{abstract}
    Data augmentation is a key component to training robust models and to
    prevent overfitting in computer vision. The Fourier perspective
    ~\citep{yin2020fourier} gave us a better understanding of the processs
    behind such improvements by showing us the tradeoffs between protecting
    against corruptions in low frequency and high frequency and the limitations
    of data augmentation. These insights also raises questions about the bias of
    gradients towards low frequencies: Does the optimizer and the architecture
    of a model bias the model to rely on low frequency features. In
    this work, we investigate the impact of Gaussian data augmentation and
    adversarial training on a different set of architectures and optmizers. We
    provide an hypothesis that gradients of adversarially trained models and
    models trained on Gaussian augmented data are naturally biased towards low
    frequency features, as they contain more relevant information for
    classification. To test our hypothesis, we provide an experimental protocol
    for testing our hypothesis against different architectures and optmizers by
    computing the accuracy of the trained models on images containing either high
    frequency features or low frequency features.
\end{abstract}

\section{Introduction}
The fourrier perspective introduced by ~\citet{yin2020fourier} paved the way
for exploring the Fourier space of images' features and how these features are
used by models. This approach was used to classify features in two categories:
high frequency and low frequency features. The first category includes features
such as images' texture, and the second is related to the countours and shapes
in images, as stated in ~\citet{krishnamachari2023fourier}. Although high
frequency features are often invisible to the human eye, the Fourier
perspective showed that these features could be successfully used by
Convolution Neural Networks (CNN) in image classification. However, high
frequency features are not robust, as showed by ~\citet{zhang2019interpreting}
and yet models are often biased toward using these features. On the contrary,
low frequency features, such as shape, are often the preferred features of
adversarially trained neural networks or network trained with a Gaussian
augmented dataset. The Fourier perspective article ~\citet{yin2020fourier}
limited their experiments to the ResNet architecture. The lack of empirical
research using different architectures and optmizers prompts the question:

\emph{Does the architecture and the optimizer influence the bias toward low
frequency features in adversarially trained models and models trained on
Gaussian augmented datasets?}

% \section{Related works}
\section{Preliminaries}
We use the following notations: $\mathcal{F}: \R^{d_1 \times d_2} \to \C^{d_1
\times d_2}$ denotes the discrete Fourier transform (DFT) of an image and we
omit the dimensions of the channels, as every channel is treated independently
of the other channels. For an image of size $N \times N$, the discrete fourier
transform is defined as
\begin{equation*}
    \mathcal{F}(k,l) = \sum_{i=0}^{N-1} \sum_{j=0}^{N-1} f(i,j) \exp^{ -i2\pi \left(\frac{ki}{N}+ \frac{lj}{N}\right)},
\end{equation*}
where $f(i,j)$ the pixel at position $i,j$ of an image. When we visualize the
Fourier spectrum, we always shift the low frequency components to the center of
the spectrum and only show the magnitude of the images, not the phase.

To filter components of an image based on their frequency, we use the
methodology of ~\citet{yin2020fourier}: We set to 0 every point in the Fourier
spectrum that is not in the square of width $B$ centered at the highest/lowest
frequency. We then apply the inverse DFT to recover the original image, with
the low/high frequency components filtered out.

Our Gaussian augmentation method follows the methodology of
~\citet{yin2020fourier}. We assume that pixels take values in the range
$[0,1]$. Pixel values are always clipped to that range. Gaussian data
augmentation with parameters $\sigma$ is defined as the following operation:
i.i.d. Gaussian noise $\mathcal{N}(0, \tilde{\sigma}^2)$, is applied at each
iteration and at every pixel. The value of $\tilde{\sigma}^2$ is chosen
uniformly at random from $[0,\sigma]$.


\section{Problem statement and related works}
Our goal is to assert whether or not optimizers and architecture influence the
bias in the features selection of models trained with a Gaussian augmented
dataset or adversarially trained. Precedent works have tried to formalize the
Fourier sensitivity of CNN with the works of ~\citet{krishnamachari2023fourier}
and gave experimental insights regarding the tunining of models towards certain
frequencies, thanks to the works of ~\citet{krishnamachari2023fourier,
geirhos2022imagenettrained, yin2020fourier, mo2022adversarial}. Notably, the
works of ~\citet{park2022vision} demonstrated that the multi-head
self-attentions, such as ViT reduce high frquency signals, while CNN amplify
them.

\section{Proposed experimental protocol}
We are planning on using two different architectures for our experiments: the
ALL-CNN architecture ~\citet{springenberg2015allcnn}, which is mostly
convolution layers stacked on top of each others, and the mobileViT architecture,
which introduce some transformer modules into the architecture ~\citet{mehta2022mobilevit}. We do not
include any pure ViT (vision transformers) models, due to their heavy
computational demands.

Our experiments consists of training a total of 12 models on the MNIST dataset
and evaluating the performance on the validation set with only high frequency
features or low frenquecy features. For both the CNN architecture and the ViT
architecture, each are trained on the default MNIST dataset, adversarially
trained on the MNIST dataset and trained on the MNIST dataset with Gaussian
augmentation. This procedure is repeated for two Stochastic gradient descent
with momentum and AdamW. Both of these optimizers were used in the original
training of ALL-CNN and MobileViT models, respectively.

To accomplish these experiments, we have access to Google Colab Pro account and
a NVDIA GTX 3060 GPU.

% \section{General formatting instructions}
% \label{gen_inst}

% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
% type with a vertical spacing (leading) of 11~points.  Times New Roman is the
% preferred typeface throughout, and will be selected for you by default.
% Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
% indentation.

% The paper title should be 17~point, initial caps/lower case, bold, centered
% between two horizontal rules. The top rule should be 4~points thick and the
% bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
% below the title to rules. All pages should start at 1~inch (6~picas) from the
% top of the page.

% For the final version, authors' names are set in boldface, and each name is
% centered above the corresponding address. The lead author's name is to be listed
% first (left-most), and the co-authors' names (if different address) are set to
% follow. If there is only one co-author, list both author and co-author side by
% side.

% Please pay special attention to the instructions in Section \ref{others}
% regarding figures, tables, acknowledgments, and references.

% \section{Headings: first level}
% \label{headings}

% All headings should be lower case (except for first word and proper nouns),
% flush left, and bold.

% First-level headings should be in 12-point type.

% \subsection{Headings: second level}

% Second-level headings should be in 10-point type.

% \subsubsection{Headings: third level}

% Third-level headings should be in 10-point type.

% \paragraph{Paragraphs}

% There is also a \verb+\paragraph+ command available, which sets the heading in
% bold, flush left, and inline with the text, with the heading followed by 1\,em
% of space.

% \section{Citations, figures, tables, references}
% \label{others}

% These instructions apply to everyone.

% \subsection{Citations within the text}

% The \verb+natbib+ package will be loaded for you by default.  Citations may be
% author/year or numeric, as long as you maintain internal consistency.  As to the
% format of the references themselves, any style is acceptable as long as it is
% used consistently.

% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations appropriate for
% use in inline text.  For example,
% \begin{verbatim}
%   \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}

% If you wish to load the \verb+natbib+ package with options, you may add the
% following before loading the \verb+neurips_2020+ package:
% \begin{verbatim}
%   \PassOptionsToPackage{options}{natbib}
% \end{verbatim}

% If \verb+natbib+ clashes with another package you load, you can add the optional
% argument \verb+nonatbib+ when loading the style file:
% \begin{verbatim}
%   \usepackage[nonatbib]{format}
% \end{verbatim}

% As submission is double blind, refer to your own published work in the third
% person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
% previous work [4].'' If you cite your other papers that are not widely available
% (e.g., a journal paper under review), use anonymous author names in the
% citation, e.g., an author of the form ``A.\ Anonymous.''

% \subsection{Footnotes}

% Footnotes should be used sparingly.  If you do require a footnote, indicate
% footnotes with a number\footnote{Sample of the first footnote.} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches (12~picas).

% Note that footnotes are properly typeset \emph{after} punctuation
% marks.\footnote{As in this example.}

% \subsection{Figures}

% \begin{figure}
%   \centering
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
% \end{figure}

% All artwork must be neat, clean, and legible. Lines should be dark enough for
% purposes of reproduction. The figure number and caption always appear after the
% figure. Place one line space before the figure caption and one line space after
% the figure. The figure caption should be lower case (except for first word and
% proper nouns); figures are numbered consecutively.

% You may use color figures.  However, it is best for the figure captions and the
% paper body to be legible if the paper is printed in either black/white or in
% color.

% \subsection{Tables}

% All tables must be centered, neat, clean and legible.  The table number and
% title always appear before the table.  See Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.

% Note that publication-quality tables \emph{do not contain vertical rules.} We
% strongly suggest the use of the \verb+booktabs+ package, which allows for
% typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.

% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \section{Final instructions}

% Do not change any aspects of the formatting parameters in the style files.  In
% particular, do not modify the width or length of the rectangle the text should
% fit into, and do not change font sizes (except perhaps in the
% \textbf{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PDF files}

% Please prepare submission files with paper size ``US Letter,'' and not, for
% example, ``A4.''

% Fonts were the main cause of problems in the past years. Your PDF file must only
% contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
% achieve this.

% \begin{itemize}

% \item You should directly generate PDF files using \verb+pdflatex+.

% \item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
%   menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
%   also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
%   available out-of-the-box on most Linux machines.

% \item The IEEE has recommendations for generating PDF files whose fonts are also
%   acceptable for NeurIPS. Please see
%   \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

% \item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
%   "solid" shapes instead.

% \item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
%   the equivalent AMS Fonts:
% \begin{verbatim}
%   \usepackage{amsfonts}
% \end{verbatim}
% followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
% for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
% workaround for reals, natural and complex:
% \begin{verbatim}
%   \newcommand{\RR}{I\!\!R} %real numbers
%   \newcommand{\Nat}{I\!\!N} %natural numbers
%   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
% \end{verbatim}
% Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.

% \end{itemize}

% If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
% you to fix it.

% \subsection{Margins in \LaTeX{}}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
% figure width as a multiple of the line width as in the example below:
% \begin{verbatim}
%   \usepackage[pdftex]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% See Section 4.4 in the graphics bundle documentation
% (\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})

% A number of width problems arise when \LaTeX{} cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
% necessary.


% \section*{Broader Impact}

% Authors are encouraged to include a statement of the broader impact of their work, including its ethical aspects and future societal consequences.
% Authors should discuss both positive and negative outcomes, if any. For instance, authors should discuss a)
% who may benefit from this research, b) who may be put at disadvantage from this research, c) what are the consequences of failure of the system, and d) whether the task/method leverages
% biases in the data. If authors believe this is not applicable to them, authors can simply state this.

% Use unnumbered first level headings for this section, which should go at the end of the report. {\bf Note that this section does not count towards the eight pages of content that are allowed.}


%\section*{References}

%References follow the acknowledgments. Use unnumbered first-level heading for
%the references. Any choice of citation style is acceptable as long as you are
%consistent. It is permissible to reduce the font size to \verb+small+ (9 points)
%when listing the references.


%\small

%[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
%connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
%(eds.), {\it Advances in Neural Information Processing Systems 7},
%pp.\ 609--616. Cambridge, MA: MIT Press.

%[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
%  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
%TELOS/Springer--Verlag.

%[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
%recall at excitatory recurrent synapses and cholinergic modulation in rat
%hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.

\bibliography{extended_abstract}
\bibliographystyle{abbrvnat}

\end{document}
